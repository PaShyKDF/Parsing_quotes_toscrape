* **Что было сделано:**  
Проанализирован сайт: динамического типа или статического; просмотрена структура, найдены нужные теги и классы с данными. Далее выбран инструмент для парсинга.
* **Откуда были получены данные:**  
Со всех страниц с цитатами.
* **Как осуществлялся сбор:**  
При помощи dev tools найдены интересущие теги. Далее настроена сборка данных с одной страницы и переключение на следующую. После настроен вывод данных в json файлы
* **Почему был выбран тот или иной метод/инструмент, а не другой:**  
Так как сайт статический и задание требует вывода собранных данный в json был выбран фреймворк Scrapy. Также приятным бонусом является то, что фреймвор работает асинхронно и кэширует страницы. За счет этого настройка и работа парсера становится быстрее и сайты не банят из-за большого количества запросов.

## Использование
Клонировать репозиторий и перейти в него в командной строке:

```bash
git clone git@github.com:PaShyKDF/Parsing_quotes_toscrape.git
```

Cоздать и активировать виртуальное окружение:

```bash
python3 -m venv venv
```

* Если у вас Linux/macOS

    ```bash
    source venv/bin/activate
    ```

* Если у вас windows

    ```bash
    source venv/scripts/activate
    ```

Установить зависимости из файла requirements.txt:

```bash
python3 -m pip install --upgrade pip
```

```bash
pip install -r requirements.txt
```

Запустить сбор данных:

```bash
scrapy crawl quotes_toscrape_spider
```